{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Sentiment Analyzer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p7NnDaYC7z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkJbUxezj8Lc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "3da942b2-b666-4d63-fb92-a439fba977b7"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/b4/1a3da73498960866ad0510ead86b133569ff012bf1c77d82ce95203779fc/bert-for-tf2-0.13.2.tar.gz (40kB)\n",
            "\r\u001b[K     |████████▏                       | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 3.6MB/s \n",
            "\u001b[?25hCollecting py-params>=0.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/17/71c5f3c0ab511de96059358bcc5e00891a804cd4049021e5fa80540f201a/py-params-0.8.2.tar.gz\n",
            "Collecting params-flow>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/12/2604f88932f285a473015a5adabf08496d88dad0f9c1228fab1547ccc9b5/params-flow-0.7.4.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.13.2-cp36-none-any.whl size=29938 sha256=fbba59730264f76059d948b4e6179e0184d7816d74bf8184c3846cd7b6d9e5b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/e1/95/7fa0b466d35f4280a8842a6653f9cd37f89e83832770daf85f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.8.2-cp36-none-any.whl size=4633 sha256=b78d02a0ef0c68278f60969fcbd2f2ab7aa521c6ba629f2e716df93711be45f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/3a/9c/baf35d6f17f0c2c6b61bf8ac3ab9fc12df0e41432ccaeecacb\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.7.4-cp36-none-any.whl size=16196 sha256=4ab09bce5525184832d37b69aa86b8f283c7b4e775949cbd498d950b30c32448\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/30/40/507b60d68b67ac87f35e95c98f5b296a32f146d5ae1d1d5aa7\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.13.2 params-flow-0.7.4 py-params-0.8.2\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4kbfSigkK6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b57d49ae-d3e1-4bba-e19a-7222ec000417"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoICukyjkNeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "dff6d46a-d8e2-4fbc-dc07-6316193e9828"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raE4M_mXkRAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"/content/drive/My Drive/ML/BERTSentimentAnalysis/Train.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a84Jjm2klpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUurHOOSlgsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9bd7b043-6ac0-4791-95e8-2064e887f274"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y92UciGliS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Removing the @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Removing the URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Keeping only letters\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Removing additional whitespaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OTwLN3xnO4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHoqGBbknkQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b6568152-bc21-4d80-c42b-6ab02935166a"
      },
      "source": [
        "data.sentiment"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0\n",
              "          ..\n",
              "1599995    4\n",
              "1599996    4\n",
              "1599997    4\n",
              "1599998    4\n",
              "1599999    4\n",
              "Name: sentiment, Length: 1600000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goDs4oGInP0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVAksyl6oxG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOyzU9SXp8Ul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a982fc6-f621-473f-abdf-d2b2017b2850"
      },
      "source": [
        "tokenizer.tokenize(\"hey... how are you\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hey', '.', '.', '.', 'how', 'are', 'you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCKpjOaqqXde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2f251f6-775a-4c0c-b2e2-26ce84be5231"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"hey... how are you\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4931, 1012, 1012, 1012, 2129, 2024, 2017]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ukZpYZ6qP3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77rsR8_Zq3ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjecCVB7q5cy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "e5129116-56a1-4212-c86a-175cd31dfaed"
      },
      "source": [
        "data_inputs[0]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22091,\n",
              " 2860,\n",
              " 2860,\n",
              " 2008,\n",
              " 1005,\n",
              " 1055,\n",
              " 1037,\n",
              " 26352,\n",
              " 5017,\n",
              " 1012,\n",
              " 2017,\n",
              " 2323,\n",
              " 2050,\n",
              " 2288,\n",
              " 2585,\n",
              " 12385,\n",
              " 1997,\n",
              " 2353,\n",
              " 2154,\n",
              " 2000,\n",
              " 2079,\n",
              " 2009,\n",
              " 1012,\n",
              " 1040]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvLpuMoysbY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMN7Pm7ovHJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb362bf3-646c-4838-f972-0c035323ab88"
      },
      "source": [
        "sorted_all[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2006, 2026, 2126, 1012, 2097, 8439, 21250, 999], 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi1lcQ9DvKS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eSmqlUhvXN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "35165434-969f-4ab9-c7aa-10d6636184f1"
      },
      "source": [
        "next(iter(all_dataset))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=int32, numpy=\n",
              " array([ 2006,  2026,  2126,  1012,  2097,  8439, 21250,   999],\n",
              "       dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZavQcwdvcJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN9rNqqjyjoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "baf75767-f98e-47f1-b57d-908a7d453a3b"
      },
      "source": [
        "next(iter(all_batched))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n",
              " array([[ 2006,  2026,  2126,  1012,  2097,  8439, 21250,   999],\n",
              "        [ 2204,  2851,  2054,  2017,  2039,  2000,  2651,  1029],\n",
              "        [ 4372,  7913, 11365,  2696,  4012, 14833,  2022,  7474],\n",
              "        [ 1045,  4771,  1996,  2203,  1997,  1045,  2293,  2769],\n",
              "        [ 2106,  1045,  2360,  1045,  8224,  4400,  2664,  1029],\n",
              "        [ 8840,  2140,  1045,  2079,  6449,  1045,  2360,  2008],\n",
              "        [ 1045,  2074, 11323,  2049,  2025,  2041, 18681,  9432],\n",
              "        [25247,  2043,  2000,  2192,  1999,  2026,  8403,  2482],\n",
              "        [ 1045,  3246,  2017,  2134,  1005,  1056,  1012,  1012],\n",
              "        [11839,  2008, 14383,  2080,  2097,  2022,  2092,  2574],\n",
              "        [ 2220,  6928,  2307, 16373,  6928,  2025,  2061,  2307],\n",
              "        [ 3666, 25742,  5092,  2497,  6229,  1045,  2991,  6680],\n",
              "        [ 2042,  6933,  1045,  2342,  7967,  1998,  3256,  6949],\n",
              "        [17852, 13433,  3070,  2009,  2003,  3492, 18370,  1012],\n",
              "        [ 2092,  2589,  2354,  2017,  1005,  1040,  2191,  2009],\n",
              "        [ 1037, 11414,  7401,  1012, 24946,  2024,  2025, 11770],\n",
              "        [ 2643,  1045,  2963, 20675,  2153,  4689, 26594,  6853],\n",
              "        [ 1045,  2215,  2000,  3942,  9424,  2153,  1012, 10231],\n",
              "        [ 1045,  1005,  1049,  3141,  2000,  2198, 15355,  2386],\n",
              "        [ 2003,  3666,  3233,  2011,  2033,  2007,  1996,  2567],\n",
              "        [ 6289,  4763,  2388,  2003,  7743,  2075,  2153,   999],\n",
              "        [ 2025,  1054,  2135,  2012,  2560,  1057,  1054,  2667],\n",
              "        [ 5622,  5054,  2378,  2000,  5061, 15854,   999,   999],\n",
              "        [22091,  2860,  2860,  2672,  2203,  1997,  2621,  1029],\n",
              "        [ 1039,  1005, 12256,  2123,  1005,  1056,  2360,  2008],\n",
              "        [ 1045,  2572,  7501,  2021,  5305,  2100,  2004,  2092],\n",
              "        [ 3398,  1045,  3214,  2663,  1012,  8246,  6297,  1012],\n",
              "        [ 2017,  1005,  2128,  2087,  6160, 10556,  7952,  1012],\n",
              "        [ 3110,  2066, 12964,  2021,  2031,  2498,  2000,  8439],\n",
              "        [ 2049,  2980,  1998,  1045,  2031,  2000,  2079, 14533],\n",
              "        [ 4826,  2003,  2026,  2197,  2154,  1997,  2104, 16307],\n",
              "        [ 2051,  2005,  2147,  2021,  2059,  2049,  5613,  2051]],\n",
              "       dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
              "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14LRHaeuyqzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzeiQGISzioN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5313d7db-cb7c-4ad1-8e6d-0a910eff82c4"
      },
      "source": [
        "NB_BATCHES"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41328"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUOg0J8_zkgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff6a4c9c-9789-44dd-9cce-09cbfca1f987"
      },
      "source": [
        "NB_BATCHES_TEST"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4132"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnhD0Eghzm0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR-n8tKd2P2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p83fw-CU3-DX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsV7gEZ54Amp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlPP55aR4Dvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed89f4f1-416b-4adf-aee3-b6427e8c5a7a"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/ML/BERTSentimentAnalysis/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NEi-DmF4MZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQlcWNzn4Qcg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f4b22926-04a9-44fd-f131-b326a30df5c2"
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  37196/Unknown - 2344s 63ms/step - loss: 0.4297 - accuracy: 0.8018Checkpoint saved at /content/drive/My Drive/ML/BERTSentimentAnalysis/.\n",
            "37196/37196 [==============================] - 2344s 63ms/step - loss: 0.4297 - accuracy: 0.8018\n",
            "Epoch 2/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.3819 - accuracy: 0.8298Checkpoint saved at /content/drive/My Drive/ML/BERTSentimentAnalysis/.\n",
            "37196/37196 [==============================] - 2381s 64ms/step - loss: 0.3819 - accuracy: 0.8298\n",
            "Epoch 3/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8512Checkpoint saved at /content/drive/My Drive/ML/BERTSentimentAnalysis/.\n",
            "37196/37196 [==============================] - 2278s 61ms/step - loss: 0.3418 - accuracy: 0.8512\n",
            "Epoch 4/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8712Checkpoint saved at /content/drive/My Drive/ML/BERTSentimentAnalysis/.\n",
            "37196/37196 [==============================] - 2301s 62ms/step - loss: 0.3010 - accuracy: 0.8712\n",
            "Epoch 5/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.2629 - accuracy: 0.8887Checkpoint saved at /content/drive/My Drive/ML/BERTSentimentAnalysis/.\n",
            "37196/37196 [==============================] - 2323s 62ms/step - loss: 0.2629 - accuracy: 0.8887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3480b76a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgp1GJ5w4TPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "581339e5-8545-4cd3-db04-b14928435085"
      },
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   4132/Unknown - 52s 12ms/step - loss: 0.4207 - accuracy: 0.8301[0.4206505089904482, 0.83009136]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTMW7Oi5yFGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Ouput of the model: {}\\nPredicted sentiment: negative.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Ouput of the model: {}\\nPredicted sentiment: positive.\".format(\n",
        "            output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OjaKnBQyZF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "215026b2-16f2-48c0-be65-ea1c202e357b"
      },
      "source": [
        "get_prediction(\"This movie was pretty interesting.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ouput of the model: [[0.9531831]]\n",
            "Predicted sentiment: positive.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfndOdr5ylMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "705a171b-db49-44e6-94c4-cc19afd8da2b"
      },
      "source": [
        "get_prediction(\"i am feeling very bad\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ouput of the model: [[0.0044844]]\n",
            "Predicted sentiment: negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LVqg8WSzXBk",
        "colab_type": "text"
      },
      "source": [
        "BERT as Embedding Layer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLdyLRf8zxlk",
        "colab_type": "text"
      },
      "source": [
        "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW4mO_s8ze7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSgwoqPR0S8a",
        "colab_type": "text"
      },
      "source": [
        "We only use the first sentence for BERT inputs so we add the CLS token at the beginning and the SEP token at the end of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLvoptNVzuMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCCsBkBk0WQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3hgioBY0cOa",
        "colab_type": "text"
      },
      "source": [
        "We need to create the 3 different inputs for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXVsjaJX0YPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSP6VoFj0bMI",
        "colab_type": "text"
      },
      "source": [
        "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bwrcE2u0kPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [([get_ids(sent_lab[0]),\n",
        "                get_mask(sent_lab[0]),\n",
        "                get_segments(sent_lab[0])],\n",
        "               sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBXub8Y80mQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A list is a type of iterator so it can be used as generator for a dataset\n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEr2m7Fb0oZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE,\n",
        "                                       padded_shapes=((3, None), ()),\n",
        "                                       padding_values=(0, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toXuLH_j0rnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BZTVmcY0tsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCNNBERTEmbedding(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=False)\n",
        "\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def embed_with_bert(self, all_tokens):\n",
        "        _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
        "                                   all_tokens[:, 1, :],\n",
        "                                   all_tokens[:, 2, :]])\n",
        "        return embs\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embed_with_bert(inputs)\n",
        "\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A463FyJu0zrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3XWMQGC0197",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
        "                         FFN_units=FFN_UNITS,\n",
        "                         nb_classes=NB_CLASSES,\n",
        "                         dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npGXKwzF03xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rP6b3UK05tK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/projects/BERT/ckpt_bert_embedding/cp-{epoch:02d}.hdf5\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STeW4W3C0-9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGoJmhRT1E2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3e3e1447-168c-4aa6-8f95-4c56090d3dd8"
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  40623/Unknown - 4359s 107ms/step - loss: 0.3968 - accuracy: 0.8221Checkpoint saved at ./drive/My Drive/projects/BERT/ckpt_bert_embedding/cp-{epoch:02d}.hdf5.\n",
            "40623/40623 [==============================] - 4362s 107ms/step - loss: 0.3968 - accuracy: 0.8221\n",
            "Epoch 2/5\n",
            "19708/40623 [=============>................] - ETA: 27:58 - loss: 0.3492 - accuracy: 0.8494Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1qvr_Be18s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}